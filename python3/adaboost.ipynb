{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集有足够的间隔，虽比较分散，但相对容易收敛\n",
    "def load_simp_data2():\n",
    "    dataMat = matrix(array([1, 5,2 ,2,3 ,1,4, 6,6 ,8,6 ,5,7 ,9,8, 7,9 ,8,10, 2]).reshape(-1,2))\n",
    "    classLabels = [1, 1, -1, -1, 1, -1, 1, 1, -1, -1]\n",
    "    \n",
    "    return dataMat ,classLabels\n",
    "\n",
    "h\n",
    "# 数据集的数据间隔不大，对数据集的划分要足够小，否则不会收敛\n",
    "def load_simp_data1():\n",
    "    dataMat = matrix([\n",
    "             [1., 2.1]\n",
    "            ,[2., 1.1]\n",
    "            ,[1.3, 1.]\n",
    "            ,[1. , 1.]\n",
    "            ,[2. , 1.]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        ])\n",
    "    classLabels = [1.0, 1.0, -1.0, -1.0, 1.0]\n",
    "    return dataMat,classLabels\n",
    "\n",
    "def stumpclassify(dataMatrix,dimen,threshVal,threshIneq):\n",
    "# 实现弱分类器功能\n",
    "    '''\n",
    "    dataMatrix:  输入的数据集\n",
    "    dimen：      数据集的特征值所在列\n",
    "    threshVal:   特征值所在列的判断阈值\n",
    "    threshIneq:  阈值的边界方向 大于/小于等于\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    retArray = ones((shape(dataMatrix)[0], 1))\n",
    "    if threshIneq == 'lt' :\n",
    "        retArray[dataMatrix[:, dimen] <= threshVal] = -1.0\n",
    "    else :\n",
    "        retArray[dataMatrix[:, dimen] > threshVal ] = -1.0\n",
    "    return retArray\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def buildStump(dataArr, classLabels, D):\n",
    "    '''\n",
    "    此函数返回该数据集中错误率最少的弱分类器的参数（最佳特征值的列，该列的阈值，阈值的方向（大于还是小于),错误率，当前轮的最佳预测值\n",
    "    \n",
    "    dataArr:      输入的数据集\n",
    "    classLabels:  输入数据集的标签\n",
    "    D          :  数据集中每个特征列的权重\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    dataMatrix = mat(dataArr)\n",
    "    labelMat = mat(classLabels).T\n",
    "    m, n = shape(dataMatrix)\n",
    "    # 修改后自动计算步长\n",
    "    dataMin = dataMatrix.min()\n",
    "    dataMax = dataMatrix.max()\n",
    "    numSteps = int( dataMax - dataMin)\n",
    "    if numSteps <10:\n",
    "        numSteps = numSteps*m\n",
    "    else:\n",
    "        numSteps = numSteps*2\n",
    "    print('numSteps is :{}'.format(numSteps))\n",
    "#     numSteps = 10.0\n",
    "    bestStump = {}\n",
    "    bestClasEst = mat(zeros((m,1))) \n",
    "    minError = inf\n",
    "    for i in range(n):\n",
    "        rangeMin = dataMatrix[:,i].min()\n",
    "        rangeMax = dataMatrix[:,i].max()\n",
    "        stepSize = (rangeMax - rangeMin) / numSteps\n",
    "        for j in range(-1,int(numSteps)+1):\n",
    "            for inequal in ['lt','gt']:\n",
    "                threshVal = (rangeMin + float(j) * stepSize)\n",
    "                predictedVals = stumpclassify(dataMatrix, i, threshVal,inequal)\n",
    "                errArr = mat(ones((m,1)))\n",
    "                errArr[predictedVals == labelMat] = 0\n",
    "                weightedError = D.T * errArr\n",
    "#                 print('split: dim {}, thresh {}, thresh ineqal:{}, the weighted error is {}'.format(i,threshVal,inequal,weightedError))\n",
    "                if weightedError < minError :\n",
    "                    minError = weightedError\n",
    "                    bestClasEst = predictedVals.copy()\n",
    "                    bestStump['dim'] = i     #特征维度\n",
    "                    bestStump['thresh'] = threshVal # 特征阈值\n",
    "                    bestStump['ineq'] = inequal  # 判断标准 大于/小于\n",
    "                    bestStump['errcount'] = np.count_nonzero(errArr)\n",
    "#     print('bestStump :{}, minError:{}, bestClasEst :{}'.format(bestStump,minError,bestClasEst))\n",
    "    return bestStump ,minError, bestClasEst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 弱分类器的权重alpha的计算公式:\n",
    "> 1、弱分类器的阈值选择是有讲究的，阈值的选择要能够分隔相邻的不同类数据。这时对步长设置的要求越高。  \n",
    "2、数据直接间隔越小，阈值要选的越精细，步长设置要使得阈值达到分隔不同数据的要求。例如，数据集1的数据间隔小，这时步长要设置精细些，反之数据集2的数据间隔大，对步长的要求就不高。  \n",
    "\n",
    "\n",
    "(1)、$\\alpha = \\frac{1}{2}ln(\\frac{1-\\varepsilon}{\\varepsilon})$ \n",
    ">1、当一个弱分类器的分类正确率为50%时，$\\alpha = 0$，所以一般弱分类器的正确率要求高于50%。  \n",
    "2、公式中的分类错误率$\\varepsilon$是分类器权重$\\alpha$累加影响下的数值，而不是特征值预测值错误个数比例。  \n",
    "3、正确率越高的弱分类器$\\alpha$系数越大，对分类错误的特征值累加结果的纠偏能力就越强。  \n",
    "4、adaboost算法，除了第一轮中D是根据样本数量均分的，其余每轮都追求将上一轮错误分类的样本在本轮（对上轮分类错误的样本算法会在本轮增大错误样本点的权重D，算法是求最小错误率，所以上轮的错误样本会在本轮优先进行纠正）进行正确分类，且追求在本轮中分类正确。所以一般都是后面轮错误率$\\varepsilon$较小，这样就使得错误率较小的弱分类器，其对应的$\\alpha$值较大。  \n",
    "\n",
    "\n",
    "#### 弱分类器中每个样本点的权重D的计算公式：\n",
    "利用预测值和原标签值的点积$ y' \\cdot y \\cdot (-a)$，如果两者相同(正确分类的样本)，结果为$(+1)*(-a)$,如果两者不同(分类错误的样本)，结果为\n",
    "$(-1) * (-a) = a $\n",
    "\n",
    "> 1、对正确分类的样本，下一轮分类时样本的权重D值较小；分类不正确的样本，下一轮分类时的样本的权重值较大。  \n",
    "2、弱分类器在找本轮最佳分类器的参数时（维度，阈值，阈值方向），就是以计算权重后的样本错误率为参考的，越小的错误率越佳；所以，上一轮分类错误的样本点一般会在本轮的算法下得到最大程度的纠正。  \n",
    "3、\n",
    "\n",
    "\n",
    "(2.1)、正确分类样本的权重：$D_i^{(t+1)} = \\frac{D_i^{(t)} e^{-\\alpha}}{Sum(D)}$\n",
    "\n",
    "(2.2)、未正确分类样本的权重：$D_i^{(t+1)} = \\frac{D_i^{(t)} e^{\\alpha}}{Sum(D)}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboostTrainDS(dataArr, classLabels,numIt = 40):\n",
    "    weakClassArr = []\n",
    "    m = shape(dataArr)[0]\n",
    "    # 修改后自动初始化权重\n",
    "    D = mat(ones((m,1)))/m\n",
    "    aggClassEst = mat(zeros((m,1)))\n",
    "    for i in range(numIt) :\n",
    "        count = i + 1\n",
    "        print('*'*80)\n",
    "        print('This is No.{}...'.format(count))\n",
    "        bestStump, error, classEst = buildStump(dataArr, classLabels,D)\n",
    "        print('D: {}'.format(D.T))\n",
    "        print('bestStump :{}, minError:{}, bestpreditVals:{}'.format(bestStump,error,classEst))\n",
    "#         print('classEst: {}'.format(classEst.T))\n",
    "        \n",
    "        alpha = float((1/2)*log((1.0-error)/max(error, 1e-16)))\n",
    "        bestStump['alpha'] = alpha\n",
    "        weakClassArr.append(bestStump)\n",
    "        print('Adjust alpha: {}'.format(alpha))\n",
    "        expon = multiply(-1*alpha*mat(classLabels).T,classEst)\n",
    "        D     = multiply(D,exp(expon))\n",
    "        print('before adjust D: {}, D.sum() = {}'.format(D.T,D.sum()))\n",
    "        D     = D/D.sum()\n",
    "        print('D/D.sum() Adjust D: {}, D.sum() ={}'.format(D.T,D.sum()))\n",
    "        \n",
    "        aggClassEst += classEst*alpha\n",
    "        print('No.{} aggclassEst: {}'.format(count,aggClassEst.T))\n",
    "        aggErrors = multiply(sign(aggClassEst)!= mat(classLabels).T, ones((m,1)) )\n",
    "        errorRate = aggErrors.sum()/m\n",
    "        print('No.{} aggclassEst error rate is: {}'.format(count,errorRate))\n",
    "        if errorRate == 0 :\n",
    "            break\n",
    "    return weakClassArr\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "This is No.1...\n",
      "numSteps is :5\n",
      "D: [[ 0.2  0.2  0.2  0.2  0.2]]\n",
      "bestStump :{'dim': 0, 'errcount': 1, 'ineq': 'lt', 'thresh': 1.3999999999999999}, minError:[[ 0.2]], bestpreditVals:[[-1.]\n",
      " [ 1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [ 1.]]\n",
      "Adjust alpha: 0.6931471805599453\n",
      "before adjust D: [[ 0.4  0.1  0.1  0.1  0.1]], D.sum() = 0.7999999999999999\n",
      "D/D.sum() Adjust D: [[ 0.5    0.125  0.125  0.125  0.125]], D.sum() =1.0000000000000002\n",
      "No.1 aggclassEst: [[-0.69314718  0.69314718 -0.69314718 -0.69314718  0.69314718]]\n",
      "No.1 aggclassEst error rate is: 0.2\n",
      "********************************************************************************\n",
      "This is No.2...\n",
      "numSteps is :5\n",
      "D: [[ 0.5    0.125  0.125  0.125  0.125]]\n",
      "bestStump :{'dim': 1, 'errcount': 1, 'ineq': 'lt', 'thresh': 1.0}, minError:[[ 0.125]], bestpreditVals:[[ 1.]\n",
      " [ 1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]]\n",
      "Adjust alpha: 0.9729550745276565\n",
      "before adjust D: [[ 0.18898224  0.04724556  0.04724556  0.04724556  0.33071891]], D.sum() = 0.6614378277661478\n",
      "D/D.sum() Adjust D: [[ 0.28571429  0.07142857  0.07142857  0.07142857  0.5       ]], D.sum() =1.0\n",
      "No.2 aggclassEst: [[ 0.27980789  1.66610226 -1.66610226 -1.66610226 -0.27980789]]\n",
      "No.2 aggclassEst error rate is: 0.2\n",
      "********************************************************************************\n",
      "This is No.3...\n",
      "numSteps is :5\n",
      "D: [[ 0.28571429  0.07142857  0.07142857  0.07142857  0.5       ]]\n",
      "bestStump :{'dim': 0, 'errcount': 2, 'ineq': 'lt', 'thresh': 0.80000000000000004}, minError:[[ 0.14285714]], bestpreditVals:[[ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]]\n",
      "Adjust alpha: 0.8958797346140273\n",
      "before adjust D: [[ 0.11664237  0.02916059  0.17496355  0.17496355  0.20412415]], D.sum() = 0.6998542122237652\n",
      "D/D.sum() Adjust D: [[ 0.16666667  0.04166667  0.25        0.25        0.29166667]], D.sum() =1.0\n",
      "No.3 aggclassEst: [[ 1.17568763  2.56198199 -0.77022252 -0.77022252  0.61607184]]\n",
      "No.3 aggclassEst error rate is: 0.0\n"
     ]
    }
   ],
   "source": [
    "datamat,labels = load_simp_data1()\n",
    "classifierArr = adaboostTrainDS(datamat,labels,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adaClassify(dataToClass, classifierArr):\n",
    "    datamat = mat(dataToClass)\n",
    "    m = datamat.shape[0]\n",
    "    aggClassEst = zeros((m,1))\n",
    "    \n",
    "    for i in range(len(classifierArr)):\n",
    "        classEst = stumpclassify(datamat\n",
    "                                ,classifierArr['dim']\n",
    "                                ,classifierArr['thresh']\n",
    "                                ,classifierArr['ineq']\n",
    "                                \n",
    "                                )\n",
    "        aggClassEst += aggClassEst + classEst * classifierArr['alpha']\n",
    "        print('The No.{} classEst is {} '.format(i+1, aggClassEst))\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
